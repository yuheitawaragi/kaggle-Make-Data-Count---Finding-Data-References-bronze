{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a58463",
   "metadata": {
    "papermill": {
     "duration": 0.004429,
     "end_time": "2025-08-29T04:45:07.613364",
     "exception": false,
     "start_time": "2025-08-29T04:45:07.608935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üßë‚Äçüî¨ Make Data Count: Competition-Leading Pipeline\n",
    "\n",
    "This notebook implements a modular pipeline for extracting and classifying research data citations from PDFs (and, optionally, XMLs), following the unique demands and quirks of the Make Data Count competition.\n",
    "\n",
    "---\n",
    "\n",
    "## Competition Context\n",
    "\n",
    "- **Goal**: Identify data mentions in research articles and classify the *relationship* to the paper as **Primary** (data generated in this study) or **Secondary** (reused data).\n",
    "- **Challenge**: Labels are *noisy and incomplete*, with major updates and forum debates (see: train labels, ‚ÄúMissing‚Äù type, new F1 scoring).\n",
    "- **Key insight**: Regex + LLM hybrid approaches, with layered filtering, currently outperform end-to-end NER or deep learning models due to label quality.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Steps\n",
    "\n",
    "### 1. Environment Setup\n",
    "- Cleans up conflicting packages (e.g. TensorFlow) and pins versions for parsing, LLM inference, and regex support.\n",
    "- Defensive: Handles CUDA, logging, and IO quirks on Kaggle.\n",
    "\n",
    "### 2. Helpers & Logging\n",
    "- Configures paths for Kaggle/notebook runs, sets up robust logging, and normalizes all text (including Unicode, stray whitespace, and weird publisher characters).\n",
    "\n",
    "### 3. PDF (and XML) Parsing\n",
    "- Converts each PDF to raw text using PyMuPDF.\n",
    "- *Forum wisdom*: Both PDF and XML are needed, as neither alone is complete or fully labeled.\n",
    "\n",
    "### 4. Extraction Engine\n",
    "- **Regex Patterns**: Finds DOIs, accessions, and other dataset IDs using hand-tuned, aggressively expanded patterns.\n",
    "- **Section Splitting**: Identifies and splits out the References section to reduce noise and false positives.\n",
    "- **Deduplication & Filters**: Removes non-dataset IDs, known bad prefixes, and article IDs.\n",
    "\n",
    "### 5. Context Window Construction\n",
    "- For each candidate ID, builds a snippet (window) of surrounding text. This is crucial for LLM classification.\n",
    "\n",
    "### 6. LLM-Based Classification\n",
    "- Uses a strongly-structured prompt and vLLM (Qwen 32B) to decide if an ID is \"Data\" or \"Literature\" (A/B), leveraging explicit prefix rules and a few-shot template.\n",
    "- Constrains output to single-character ‚ÄúA‚Äù or ‚ÄúB‚Äù for reliability.\n",
    "\n",
    "### 7. Post-Filtering\n",
    "- Drops IDs with common literature/publisher prefixes unless ‚Äúdata context‚Äù words are present in the window.\n",
    "- Aggressive final deduplication.\n",
    "\n",
    "### 8. Scoring & Output\n",
    "- Outputs submission as required (article_id, dataset_id, type).\n",
    "- Local F1 scoring mimics the updated official metric: \"Missing\" labels only penalize explicit mistakes; others use classic F1.\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Works\n",
    "\n",
    "- **Sane Extraction**: Regex + layered filtering catches nearly all valid IDs, especially with hand-crafted postprocessing.\n",
    "- **LLM for Context**: Prompt engineering beats naive NER for deciding ‚Äúdata‚Äù vs ‚Äúliterature,‚Äù especially with incomplete labels.\n",
    "- **Flexible**: Each stage is isolated for quick patching‚Äîkey in a noisy, changing competition.\n",
    "\n",
    "---\n",
    "\n",
    "*If you have a regex improvement, please fork and share. However, I think the real gold will lie in creating a dataset that maps these regexes to other \"aliases\" of datasets. \n",
    "\n",
    "May the F1 be ever in your favor!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5e1896",
   "metadata": {
    "_cell_guid": "eae4b221-a822-451f-8f4b-134c3f9bfe2c",
    "_uuid": "b1883565-f717-4130-a662-5bb541f45ea1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-29T04:45:07.620933Z",
     "iopub.status.busy": "2025-08-29T04:45:07.620698Z",
     "iopub.status.idle": "2025-08-29T04:45:29.763343Z",
     "shell.execute_reply": "2025-08-29T04:45:29.762505Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 22.14785,
     "end_time": "2025-08-29T04:45:29.764744",
     "exception": false,
     "start_time": "2025-08-29T04:45:07.616894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 5.07s\u001b[0m\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mtensorflow\u001b[0m\u001b[2m==2.18.0\u001b[0m\r\n",
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\r\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m157 packages\u001b[0m \u001b[2min 509ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m52 packages\u001b[0m \u001b[2min 15.29s\u001b[0m\u001b[0m\r\n",
      "\u001b[2mUninstalled \u001b[1m14 packages\u001b[0m \u001b[2min 231ms\u001b[0m\u001b[0m\r\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m52 packages\u001b[0m \u001b[2min 93ms\u001b[0m\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mairportsdata\u001b[0m\u001b[2m==20250622\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mastor\u001b[0m\u001b[2m==0.8.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mblake3\u001b[0m\u001b[2m==1.0.5\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mcompressed-tensors\u001b[0m\u001b[2m==0.9.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdepyf\u001b[0m\u001b[2m==0.18.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiskcache\u001b[0m\u001b[2m==5.6.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mfastapi-cli\u001b[0m\u001b[2m==0.0.7\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mgguf\u001b[0m\u001b[2m==0.17.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttptools\u001b[0m\u001b[2m==0.6.4\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.7.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.0.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1minteregular\u001b[0m\u001b[2m==0.3.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mllguidance\u001b[0m\u001b[2m==0.7.30\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.43.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mllvmlite\u001b[0m\u001b[2m==0.44.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlm-format-enforcer\u001b[0m\u001b[2m==0.10.11\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mlogits-processor-zoo\u001b[0m\u001b[2m==0.1.12\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mmistral-common\u001b[0m\u001b[2m==1.6.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsgspec\u001b[0m\u001b[2m==0.19.0\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.60.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumba\u001b[0m\u001b[2m==0.61.2\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.5.3.2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.4.5.8\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.3.0.75\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.1.0.70\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.3.61\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.2.1.3\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.6.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.5.147\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.3.83\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.6.1.9\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.1.3\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.3.1.170\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.5.82\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.4.127\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-api\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-common\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-grpc\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-exporter-otlp-proto-http\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-proto\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-sdk\u001b[0m\u001b[2m==1.26.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions\u001b[0m\u001b[2m==0.47b0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mopentelemetry-semantic-conventions-ai\u001b[0m\u001b[2m==0.4.9\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines\u001b[0m\u001b[2m==0.1.11\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1moutlines-core\u001b[0m\u001b[2m==0.1.26\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpartial-json-parser\u001b[0m\u001b[2m==0.2.1.1.post6\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mprometheus-fastapi-instrumentator\u001b[0m\u001b[2m==7.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycountry\u001b[0m\u001b[2m==24.6.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpymupdf\u001b[0m\u001b[2m==1.26.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\r\n",
      " \u001b[31m-\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==24.0.1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.0.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mrich-toolkit\u001b[0m\u001b[2m==0.14.7\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1muvloop\u001b[0m\u001b[2m==0.21.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mvllm\u001b[0m\u001b[2m==0.8.5.post1\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mwatchfiles\u001b[0m\u001b[2m==1.1.0\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.29.post2\u001b[0m\r\n",
      " \u001b[32m+\u001b[39m \u001b[1mxgrammar\u001b[0m\u001b[2m==0.1.18\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! uv pip uninstall --system 'tensorflow'\n",
    "! uv pip install --system --no-index --find-links='/kaggle/input/latest-mdc-whls/whls' 'pymupdf' 'vllm' 'triton' 'logits-processor-zoo' 'numpy<2'\n",
    "! mkdir -p /tmp/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb5ce9f3",
   "metadata": {
    "_cell_guid": "34135540-31fa-4d24-8934-acb1e0711a4f",
    "_uuid": "92f33014-02d3-41cb-b906-ffeb89a3f353",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-29T04:45:29.780906Z",
     "iopub.status.busy": "2025-08-29T04:45:29.780653Z",
     "iopub.status.idle": "2025-08-29T04:45:29.787215Z",
     "shell.execute_reply": "2025-08-29T04:45:29.786451Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.0156,
     "end_time": "2025-08-29T04:45:29.788244",
     "exception": false,
     "start_time": "2025-08-29T04:45:29.772644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/helpers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/helpers.py\n",
    "import logging, os, kagglehub, inspect\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "\n",
    "IS_KAGGLE_ENV = sum(['KAGGLE' in k for k in os.environ]) > 0\n",
    "IS_KAGGLE_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "COMP_DIR = Path(('/kaggle/input/make-data-count-finding-data-references' if IS_KAGGLE_SUBMISSION else kagglehub.competition_download('make-data-count-finding-data-references')))\n",
    "PDF_DIR = COMP_DIR / ('test' if IS_KAGGLE_SUBMISSION else 'train') / 'PDF'\n",
    "WORKING_DIR = Path(('/kaggle/working/' if IS_KAGGLE_ENV else '.working/'))\n",
    "\n",
    "DOI_LINK = 'https://doi.org/'\n",
    "\n",
    "DEFAULT_LOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"DEBUG\").upper() if not IS_KAGGLE_SUBMISSION else \"WARNING\"\n",
    "LOG_FILE_PATH = os.getenv(\"LOG_FILE\", \"logs/project.log\")\n",
    "LOG_DIR = Path(LOG_FILE_PATH).parent\n",
    "\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_FORMAT = \"%(levelname)s %(asctime)s  [%(filename)s:%(lineno)d - %(funcName)s()] %(message)s\"\n",
    "LOG_DATEFMT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "def get_logger(name=None):\n",
    "    if name is None:\n",
    "        frame = inspect.currentframe()\n",
    "        if frame is None or frame.f_back is None:\n",
    "            name = \"__main__\"\n",
    "        else:\n",
    "            name = frame.f_back.f_globals.get(\"__name__\", \"__main__\")\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        formatter = logging.Formatter(fmt=LOG_FORMAT, datefmt=LOG_DATEFMT)\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        ch.setFormatter(formatter)\n",
    "        fh = logging.FileHandler(LOG_FILE_PATH)\n",
    "        fh.setLevel(DEFAULT_LOG_LEVEL)\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(ch)\n",
    "        logger.addHandler(fh)\n",
    "        logger.propagate = False\n",
    "    return logger\n",
    "\n",
    "def is_doi_link(name: str) -> pl.Expr:\n",
    "    return pl.col(name).str.starts_with(DOI_LINK)\n",
    "\n",
    "def string_normalization(name: str) -> pl.Expr:\n",
    "    return pl.col(name).str.normalize(\"NFKC\").str.replace_all(r\"[^\\p{Ascii}]\", '').str.replace_all(r\"https?://zenodo\\.org/record/(\\d+)\", r\" 10.5281/zenodo.$1 \")\n",
    "\n",
    "def get_df(parse_dir: str):\n",
    "    records = []\n",
    "    txt_files = list(Path(parse_dir).glob('*.txt'))\n",
    "    for txt_file in txt_files:\n",
    "        id_ = txt_file.stem\n",
    "        with open(txt_file, 'r') as f:\n",
    "            text = f.read()\n",
    "        records.append({'article_id': id_, 'text': text})\n",
    "    return pl.DataFrame(records).with_columns(string_normalization('text').alias('text'))\n",
    "\n",
    "def assume_type(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return (\n",
    "        df.with_columns(pl.when(is_doi_link('dataset_id').or_(pl.col('dataset_id').str.starts_with('SAMN'))).then(pl.lit('Primary')).otherwise(pl.lit('Secondary')).alias('type'))\n",
    "    )\n",
    "\n",
    "def score(df, gt, on, tag='all'):\n",
    "    hits = gt.join(df, on=on)\n",
    "    tp = hits.height\n",
    "    fp = df.height - tp\n",
    "    fn = gt.height - tp\n",
    "    f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n",
    "    return f\"{tag} - f1: {f1:.4f} [{tp}/{fp}/{fn}]\"\n",
    "\n",
    "def evaluate(df, on=['article_id', 'dataset_id']):\n",
    "    gt = pl.read_csv(COMP_DIR/'train_labels.csv').filter(pl.col('type')!='Missing')\n",
    "    return (\n",
    "        score(df, gt, on),\n",
    "        score(df.filter(is_doi_link('dataset_id')), gt.filter(is_doi_link('dataset_id')), on, 'doi'),\n",
    "        score(df.filter(~is_doi_link('dataset_id')), gt.filter(~is_doi_link('dataset_id')), on, 'acc'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bd43a4f",
   "metadata": {
    "_cell_guid": "98be0899-3cad-423b-a3db-313209068df0",
    "_uuid": "84859532-6acd-4011-b783-d0d24257a19b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-29T04:45:29.803379Z",
     "iopub.status.busy": "2025-08-29T04:45:29.803170Z",
     "iopub.status.idle": "2025-08-29T04:45:29.808333Z",
     "shell.execute_reply": "2025-08-29T04:45:29.807743Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.014122,
     "end_time": "2025-08-29T04:45:29.809395",
     "exception": false,
     "start_time": "2025-08-29T04:45:29.795273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/parse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/parse.py\n",
    "\"\"\"\n",
    "parser.py\n",
    "=========\n",
    "‚Ä¢ Detects competition-submission mode\n",
    "‚Ä¢ Parses both PDF and XML into plain text\n",
    "‚Ä¢ Cleans text with the same heuristics\n",
    "‚Ä¢ Writes /kaggle/working/output_dir/<article_id>.txt\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "import os, re, unicodedata, fitz               # PyMuPDF\n",
    "from tqdm.auto import tqdm\n",
    "import pymupdf\n",
    "import os, re, pathlib\n",
    "import polars as pl\n",
    "from lxml import etree\n",
    "import pymupdf\n",
    "from typing import Tuple\n",
    "\n",
    "# ----------------------- Environment / paths ---------------------------------\n",
    "IS_KAGGLE_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "SPLIT = \"test\" if IS_KAGGLE_SUBMISSION else \"train\"\n",
    "\n",
    "BASE = Path(\"/kaggle/input/make-data-count-finding-data-references\") / SPLIT\n",
    "PDF_DIR = BASE / \"PDF\"\n",
    "XML_DIR = BASE / \"XML\"\n",
    "\n",
    "OUT_DIR = Path(\"/tmp/train_parse\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# XML & PDF Parsing\n",
    "\n",
    "def xml_kind(path: pathlib.Path) -> str:\n",
    "    head = path.open('rb').read(2048).decode('utf8', 'ignore')\n",
    "    if 'www.tei-c.org/ns' in head:\n",
    "        return 'tei'\n",
    "    if re.search(r'(NLM|TaxonX)//DTD', head):\n",
    "        return 'jats'\n",
    "    if 'www.wiley.com/namespaces' in head:\n",
    "        return 'wiley'\n",
    "    if 'BioC.dtd' in head:\n",
    "        return 'bioc'\n",
    "    return 'unknown'\n",
    "\n",
    "def xml2text(path: pathlib.Path) -> str:\n",
    "    kind = xml_kind(path)\n",
    "    root = etree.parse(str(path)).getroot()\n",
    "    if kind in ('tei', 'bioc', 'unknown'):\n",
    "        txt = ' '.join(root.itertext())\n",
    "    elif kind == 'jats':\n",
    "        elems = root.xpath('//body//sec|//ref-list')\n",
    "        txt = ' '.join(' '.join(e.itertext()) for e in elems)\n",
    "    elif kind == 'wiley':\n",
    "        elems = root.xpath('//*[local-name()=\"body\"]|//*[local-name()=\"refList\"]')\n",
    "        txt = ' '.join(' '.join(e.itertext()) for e in elems)\n",
    "    else:\n",
    "        txt = ' '.join(root.itertext())\n",
    "    txt = re.sub(r'10\\.\\d{4,9}/\\s+', '10.', txt)\n",
    "    return txt\n",
    "\n",
    "def pdf2text(path: pathlib.Path, out_dir: pathlib.Path) -> None:\n",
    "    doc = pymupdf.open(str(path))\n",
    "    out = out_dir / f\"{path.stem}.txt\"\n",
    "    with open(out, \"wb\") as f:\n",
    "        for page in doc:\n",
    "            f.write(page.get_text().encode(\"utf8\"))\n",
    "            f.write(b\"\\n\")\n",
    "\n",
    "# Parse All PDFs & XMLs to TXT\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def parse_all_pdfs_xmls(pdf_dir, xml_dir, parsed_dir):\n",
    "    pdf_files = list(pdf_dir.glob('*.pdf'))\n",
    "    if not pdf_files and not xml_dir.exists():\n",
    "        raise ValueError(\"No PDF or XML files found.\")\n",
    "\n",
    "    parsed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # PDF ‚Üí TXT\n",
    "    for pdf in tqdm(pdf_files, desc=\"PDF‚ÜíTXT\"):\n",
    "        try:\n",
    "            pdf2text(pdf, parsed_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"PDF error {pdf.stem}: {e}\")\n",
    "\n",
    "    # XML ‚Üí TXT (append mode)\n",
    "    if xml_dir.exists():\n",
    "        for xml in tqdm(xml_dir.glob('*.xml'), desc=\"XML‚ÜíTXT\"):\n",
    "            try:\n",
    "                txt = xml2text(xml).encode(\"utf8\")\n",
    "                out = parsed_dir / f\"{xml.stem}.txt\"\n",
    "                with open(out, \"ab\") as f:  # 'ab' = append binary\n",
    "                    f.write(txt)\n",
    "                    f.write(b\"\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"XML error {xml.stem}: {e}\")\n",
    "    print(\"Done parsing to text.\")\n",
    "\n",
    "parse_all_pdfs_xmls(PDF_DIR, XML_DIR, OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61381b9",
   "metadata": {
    "_cell_guid": "01632e8b-2a68-4dec-9606-f91214a8c020",
    "_uuid": "5210e49f-e5ab-45c6-b673-f0bd08dc1877",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-29T04:45:29.824964Z",
     "iopub.status.busy": "2025-08-29T04:45:29.824436Z",
     "iopub.status.idle": "2025-08-29T04:45:29.829310Z",
     "shell.execute_reply": "2025-08-29T04:45:29.828567Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013859,
     "end_time": "2025-08-29T04:45:29.830517",
     "exception": false,
     "start_time": "2025-08-29T04:45:29.816658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/check_parse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/check_parse.py\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from helpers import *\n",
    "\n",
    "l=get_logger()\n",
    "\n",
    "def gt_dataset_id_normalization(name:str) -> pl.Expr:\n",
    "    return (\n",
    "        pl.when(is_doi_link(name))\n",
    "        .then(pl.col(name).str.split(DOI_LINK).list.last())\n",
    "        .otherwise(name)\n",
    "        .str.to_lowercase()\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    if IS_KAGGLE_SUBMISSION:\n",
    "        l.debug('skipping check_parse for submission')\n",
    "        return\n",
    "    df = (\n",
    "        get_df('/tmp/train_parse')\n",
    "        .with_columns(pl.col('text').str.replace_all('\\s+', '').str.to_lowercase().alias('text'))\n",
    "    )\n",
    "\n",
    "    gt = (\n",
    "        pl.read_csv(COMP_DIR/'train_labels.csv')\n",
    "        .filter(pl.col('article_id').is_in(df['article_id']))\n",
    "        .filter(pl.col('type')!='Missing')\n",
    "        .with_columns(gt_dataset_id_normalization('dataset_id').alias('norm_id'))\n",
    "    )\n",
    "\n",
    "    l.info(f\"pymupdf misses: {gt.join(df, on='article_id').with_columns(hit=pl.col('text').str.contains(pl.col('norm_id'), literal=True)).filter(~pl.col('hit')).height} dataset_ids\")\n",
    "\n",
    "if __name__=='__main__': main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4af8ec",
   "metadata": {
    "_cell_guid": "83084a3e-ab3e-4c24-9045-7bc81df72e34",
    "_uuid": "5a79e391-1a5c-4264-9aa5-747cb657a266",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-29T04:45:29.845557Z",
     "iopub.status.busy": "2025-08-29T04:45:29.845356Z",
     "iopub.status.idle": "2025-08-29T04:45:29.855073Z",
     "shell.execute_reply": "2025-08-29T04:45:29.854470Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018402,
     "end_time": "2025-08-29T04:45:29.856108",
     "exception": false,
     "start_time": "2025-08-29T04:45:29.837706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/getid.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/getid.py\n",
    "import re\n",
    "import polars as pl\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "import re\n",
    "\n",
    "COMPILED_PATTERNS = {\n",
    "    # ÂèÇËÄÉÊñáÁåÆË¶ãÂá∫„Åó„ÅÆÂ§öÊßòÂåñ„Å®Á©∫ÁôΩÊñáÂ≠ó„ÅÆÊüîËªüÂØæÂøú\n",
    "    'ref_header_patterns': [\n",
    "        re.compile(r'\\b(R\\s*E\\s*F\\s*E\\s*R\\s*E\\s*N\\s*C\\s*E\\s*S|BIBLIOGRAPHY|LITERATURE CITED|WORKS CITED|CITED WORKS)\\b[:\\s]*', re.IGNORECASE),\n",
    "        re.compile(r'\\b(REFERENCES|BIBLIOGRAPHY|LITERATURE\\s*CITED|WORKS\\s*CITED|CITED\\s*WORKS)\\b', re.IGNORECASE),\n",
    "    ],\n",
    "\n",
    "    # ÂºïÁî®„Éë„Çø„Éº„É≥„ÅÆÊã°ÂºµÔºàÁï™Âè∑„É™„Çπ„Éà„ÄÅÁØÑÂõ≤„ÄÅËëóËÄÖÂêçÔºãÂπ¥„ÄÅDOI„Å™„Å©Ôºâ\n",
    "    'citation_pattern': re.compile(\n",
    "        r'^\\s*('\n",
    "        r'\\[\\d+(-\\d+)?(,\\s*\\d+(-\\d+)?)*\\]'                  # [1], [12-15], [1,3,5-7]\n",
    "        r'|\\([A-Z][a-z]+ et al\\., \\d{4}\\)'                  # (Smith et al., 2020)\n",
    "        r'|doi:\\s*10\\.\\d{4,9}/[-._;()/:A-Z0-9]+'            # doi:10.xxxx/xxxxx\n",
    "        r'|https?://doi\\.org/10\\.\\d{4,9}/[-._;()/:A-Z0-9]+' # https://doi.org/10.xxxx/xxxxx\n",
    "        r'|\\d+\\.|\\d+\\)|\\(\\d+\\)|\\d+(?=\\s|$)'                  # 1. 1) (1) 1ÔºàÂçòÁ¥îÁï™Âè∑Ôºâ\n",
    "        r')\\s*', re.IGNORECASE),\n",
    "\n",
    "    # ÊúÄÂàù„ÅÆÂºïÁî®„Éë„Çø„Éº„É≥„ÇÇÂøÖË¶Å„Å´Âøú„Åò„Å¶Êã°ÂºµÂèØËÉΩ\n",
    "    'first_citation_patterns': [\n",
    "        re.compile(r'^\\s*\\[1\\]\\s*'),\n",
    "        re.compile(r'^\\s*\\(1\\)\\s*'),\n",
    "        re.compile(r'^\\s*1\\.\\s*'),\n",
    "        re.compile(r'^\\s*1\\)\\s*'),\n",
    "        re.compile(r'^\\s*1(?=\\s|$)'),\n",
    "        # ‰æã: ËëóËÄÖÂêçÔºãÂπ¥„ÅÆÊúÄÂàù„ÅÆÂºïÁî®„Éë„Çø„Éº„É≥ËøΩÂä†„ÇÇÂèØËÉΩ\n",
    "        re.compile(r'^\\s*\\([A-Z][a-z]+ et al\\., 20\\d{2}\\)'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "def find_last_reference_header(text: str, header_patterns: list[re.Pattern]) -> Optional[int]:\n",
    "    last_match_idx = None\n",
    "    for pattern in header_patterns:\n",
    "        matches = list(pattern.finditer(text))\n",
    "        if matches:\n",
    "            last_match_idx = matches[-1].start()\n",
    "    return last_match_idx\n",
    "\n",
    "def find_last_first_citation(text: str) -> Optional[int]:\n",
    "    lines = text.splitlines()\n",
    "    last_match_line = None\n",
    "    for line_num, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        for pattern in COMPILED_PATTERNS['first_citation_patterns']:\n",
    "            if pattern.match(line):\n",
    "                next_lines = lines[line_num:line_num+3]\n",
    "                if any(COMPILED_PATTERNS['citation_pattern'].match(l.strip()) for l in next_lines[1:]):\n",
    "                    last_match_line = line_num\n",
    "                break\n",
    "    return last_match_line\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "# Ë£úÂä©Èñ¢Êï∞ÔºöË™§Ê§úÂá∫Ë°åÔºà„Éé„Ç§„Ç∫Ë°åÔºâ„ÇíÂà§ÂÆö„Åô„ÇãÈñ¢Êï∞\n",
    "def is_noise_line(line: str) -> bool:\n",
    "    stripped = line.strip()\n",
    "    if not stripped:\n",
    "        return True  # Á©∫Ë°å„ÅØ„Éé„Ç§„Ç∫„Å®„Åø„Å™„Åô\n",
    "\n",
    "    # Âå∫Âàá„ÇäÁ∑öÔºà---, ===, ***„Å™„Å©Ôºâ\n",
    "    if stripped in ['---', '===', '***']:\n",
    "        return True\n",
    "\n",
    "    # „Éö„Éº„Ç∏Áï™Âè∑„ÅÆ„Çà„ÅÜ„Å™Ë°å‰æã: \"Page 12\"\n",
    "    if stripped.lower().startswith('page ') and stripped[5:].strip().isdigit():\n",
    "        return True\n",
    "\n",
    "    # Figure„ÇÑTable„ÅÆ„É©„Éô„É´Ë°å„ÇíÈô§Â§ñÔºà‰æã: Figure 1, Table 2Ôºâ\n",
    "    if stripped.lower().startswith('figure ') or stripped.lower().startswith('table '):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def find_reference_start(text: str) -> Optional[int]:\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # ‚ë† Ë¶ãÂá∫„ÅóÊ§úÂá∫ÔºöCOMPILED_PATTERNS['ref_header_patterns']„ÅÆ„Å©„Çå„Åã„Å´„Éû„ÉÉ„ÉÅ„Åô„Çå„Å∞Ê§úÂá∫\n",
    "    for i, line in enumerate(lines):\n",
    "        for pattern in COMPILED_PATTERNS['ref_header_patterns']:\n",
    "            if pattern.search(line):\n",
    "                # Ë¶ãÂá∫„Åó„ÅÆÊ¨°Êï∞Ë°å„ÅßÁ©∫Ë°å„Éª„Éé„Ç§„Ç∫Ë°å„Çí„Çπ„Ç≠„ÉÉ„Éó„Åó„Å¶ÊúÄÂàù„ÅÆÊúâÂäπË°å„ÇíËøî„Åô\n",
    "                for offset in range(1, 6):\n",
    "                    idx = i + offset\n",
    "                    if idx >= len(lines):\n",
    "                        break\n",
    "                    candidate_line = lines[idx].strip()\n",
    "                    if candidate_line and not is_noise_line(candidate_line):\n",
    "                        return idx\n",
    "                # Ë¶ãÂá∫„ÅóÁõ¥Âæå„Å´Ë©≤ÂΩìË°å„Åå„Å™„Åë„Çå„Å∞Ë¶ãÂá∫„ÅóË°å+1„ÇíËøî„Åô\n",
    "                return i + 1\n",
    "\n",
    "    # ‚ë° Êó¢Â≠ò„ÅÆÊúÄÂæå„ÅÆÂàùÂá∫ÂºïÁî®‰ΩçÁΩÆ„ÇíË©¶„Åô\n",
    "    last_first_citation = find_last_first_citation(text)\n",
    "    if last_first_citation is not None:\n",
    "        return last_first_citation\n",
    "\n",
    "    # ‚ë¢ ÂæåÂçä„Åã„ÇâÂºïÁî®„Éë„Çø„Éº„É≥„ÅÆÈÄ£Á∂öË°å„ÇíÊé¢„Åô\n",
    "    start_search_idx = int(len(lines) * 0.5)\n",
    "    for i in range(start_search_idx, len(lines)):\n",
    "        line = lines[i].strip()\n",
    "        if is_noise_line(line) or len(line) < 5:\n",
    "            continue\n",
    "        if COMPILED_PATTERNS['citation_pattern'].match(line):\n",
    "            next_lines = lines[i:i + 5]\n",
    "            count = sum(1 for l in next_lines if COMPILED_PATTERNS['citation_pattern'].match(l.strip()))\n",
    "            if count >= 3:\n",
    "                # Áõ¥Ââç„ÅÆÂºïÁî®„Åß„ÅØ„Å™„ÅÑË°å„ÇíÊé¢„ÅôÔºàÊúÄÂ§ß15Ë°åÂâç„Åæ„ÅßÔºâ\n",
    "                for j in range(i, max(-1, i - 15), -1):\n",
    "                    prev_line = lines[j].strip()\n",
    "                    if prev_line and not COMPILED_PATTERNS['citation_pattern'].match(prev_line) and not is_noise_line(prev_line):\n",
    "                        return j + 1\n",
    "                return max(0, i - 15)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def split_text_and_references(text: str) -> Tuple[str, str]:\n",
    "    header_idx = find_last_reference_header(text, COMPILED_PATTERNS['ref_header_patterns'])\n",
    "    prev_idx = None\n",
    "    while header_idx is not None and header_idx != prev_idx:\n",
    "        prev_idx = header_idx\n",
    "        header_idx = find_last_reference_header(text[:header_idx].strip(), COMPILED_PATTERNS['ref_header_patterns'])\n",
    "    if prev_idx is not None:\n",
    "        return text[:prev_idx].strip(), text[prev_idx:].strip()\n",
    "\n",
    "    ref_start_line = find_reference_start(text)\n",
    "    if ref_start_line is not None:\n",
    "        lines = text.splitlines()\n",
    "        body = '\\n'.join(lines[:ref_start_line])\n",
    "        refs = '\\n'.join(lines[ref_start_line:])\n",
    "        return body.strip(), refs.strip()\n",
    "\n",
    "    return text.strip(), ''\n",
    "\n",
    "def get_splits(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    bodies, refs = [], []\n",
    "    for raw_text in df['text']:\n",
    "        main, ref = split_text_and_references(raw_text)\n",
    "        bodies.append(main)\n",
    "        refs.append(ref)\n",
    "    return df.with_columns(pl.Series('body', bodies), pl.Series('ref', refs))\n",
    "\n",
    "def tidy_extraction(df) -> pl.DataFrame:\n",
    "    bad_ids = [f'{DOI_LINK}{e}' for e in ['10.5061/dryad', '10.5281/zenodo', '10.6073/pasta']]\n",
    "\n",
    "    doi_df = (\n",
    "        df.with_columns(pl.col('body').str.extract_all(r'10\\s*\\.\\s*\\d{4,9}\\s*/\\s*\\S+').alias('match'))\n",
    "          .explode('match')\n",
    "          .drop_nulls('match')\n",
    "          .with_columns(\n",
    "              pl.col('match').str.replace_all(r'\\s+', '')\n",
    "                             .str.replace(r'[^A-Za-z0-9]+$', '')\n",
    "                             .str.to_lowercase()\n",
    "                             .alias('dataset_id')\n",
    "          )\n",
    "          .group_by('article_id', 'dataset_id')\n",
    "          .agg('match')\n",
    "          .with_columns((DOI_LINK + pl.col('dataset_id')).alias('dataset_id'))\n",
    "    )\n",
    "\n",
    "    REGEX_IDS = (\n",
    "    r\"(?i)\\b(?:\"\n",
    "    r\"CHEMBL\\s*\\d+|\"\n",
    "    r\"E-GEOD-\\s*\\d+|E-PROT-\\s*\\d+|E-MTAB-\\s*\\d+|E-MEXP-\\s*\\d+|EMPIAR-\\s*\\d+|\"\n",
    "    r\"ENSBTAG\\s*\\d+|ENSOARG\\s*\\d+|\"\n",
    "    r\"EPI\\s*_?\\s*ISL\\s*_?\\s*\\d{5,}|EPI\\s*\\d{6,7}|\"\n",
    "    r\"HPA\\s*\\d+|CP\\s*\\d{6}|IPR\\s*\\d{6}|PF\\s*\\d{5}|BX\\s*\\d{6}|KX\\s*\\d{6}|K0\\s*\\d{4}|CAB\\s*\\d{6}|\"\n",
    "    r\"NC\\s*_\\s*\\d{6}\\.\\d{1}|NM\\s*_\\s*\\d{9}|\"\n",
    "    r\"PRJNA\\s*\\d+|PRJDB\\s*\\d+|PRJEB\\s*\\d+|PXD\\s*\\d+|SAMN\\s*\\d+|\"\n",
    "    r\"GSE\\s*\\d+|GSM\\s*\\d+|\"\n",
    "    r\"CVCL\\s*_\\s*[A-Z0-9]{4}|\"\n",
    "    r\"PDB\\s*[1-9][A-Z0-9]{3}|HMDB\\s*\\d+|\"\n",
    "    r\"dryad\\.\\s*[^\\s\\\"<>]+|pasta\\/\\s*[^\\s\\\"<>]+|\"\n",
    "    r\"(?:SR[RPAX]|STH|ERR|DRR|DRX|DRP|ERP|ERX)\\d+|\"\n",
    "    r\"phs\\d{6}(?:\\.v\\d{1,2}\\.p\\d{1,2})?|\"   # dbGaP accession\n",
    "    r\"MTBLS\\d+|\"                            # MetaboLights\n",
    "    r\"E-[A-Z]{4}-\\d+|\"                      # ArrayExpress (general)\n",
    "    r\"ds\\d{6}|\"                             # OpenNeuro\n",
    "    r\"[1-5]\\s*\\.(?:10|20|30|40|50|60|70|80|90)\\s*\\.\\d{2,4}\\s*\\.\\d{2,4}\"  # numeric DOI-like\n",
    "    r\")\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    acc_df = (\n",
    "        df.with_columns(\n",
    "            pl.col('text').str.extract_all(REGEX_IDS).alias('match')\n",
    "        )\n",
    "        .explode('match')\n",
    "        .drop_nulls('match')\n",
    "        .with_columns(\n",
    "            pl.col('match').str.replace_all(r'\\s+', '')\n",
    "                           .str.replace(r'[^A-Za-z0-9]+$', '')\n",
    "                           .str.replace(r'(?i)^PDB', '')\n",
    "                           .alias('dataset_id')\n",
    "        )\n",
    "        .group_by('article_id', 'dataset_id')\n",
    "        .agg('match')\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('dataset_id').str.starts_with('dryad.'))\n",
    "              .then(f'{DOI_LINK}10.5061/' + pl.col('dataset_id'))\n",
    "              .otherwise('dataset_id')\n",
    "              .alias('dataset_id')\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('dataset_id').str.starts_with('pasta/'))\n",
    "              .then(f'{DOI_LINK}10.6073/' + pl.col('dataset_id'))\n",
    "              .otherwise('dataset_id')\n",
    "              .alias('dataset_id')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df = pl.concat([doi_df, acc_df])\n",
    "\n",
    "    df = (\n",
    "        df.unique(['article_id', 'dataset_id'])  # CHANGED\n",
    "          .filter(~pl.col('article_id').str.replace('_','/').str.contains(pl.col('dataset_id').str.split(DOI_LINK).list.last().str.escape_regex()))\n",
    "          .filter(~pl.col('dataset_id').str.contains(pl.col('article_id').str.replace('_','/').str.escape_regex()))\n",
    "          .filter(~pl.col('dataset_id').str.contains('figshare', literal=True))\n",
    "          .filter(~pl.col('dataset_id').is_in(bad_ids))\n",
    "          .filter(\n",
    "              pl.when(is_doi_link('dataset_id') &\n",
    "                      (pl.col('dataset_id').str.split('/').list.last().str.len_chars() < 5))\n",
    "               .then(False)\n",
    "               .otherwise(True)\n",
    "          )\n",
    "          .with_columns(pl.col('match').list.unique())\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def get_context_window(text: str, substring: str, window: int = 100) -> str:\n",
    "    idx = text.find(substring)\n",
    "    if idx == -1:\n",
    "        raise ValueError\n",
    "    start = max(idx - window, 0)\n",
    "    end = min(idx + len(substring) + window, len(text))\n",
    "    return text[start:end]\n",
    "\n",
    "def get_window_df(text_df, ids_df):\n",
    "    df = ids_df.join(text_df, on='article_id')\n",
    "    windows = []\n",
    "    for text, match_ids in df.select('text', 'match').rows():\n",
    "        windows.append(get_context_window(text, match_ids[0]))\n",
    "    return df.with_columns(pl.Series('window', windows)).select('article_id', 'dataset_id', 'window')\n",
    "\n",
    "def main():\n",
    "    text_df = get_df('/tmp/train_parse')\n",
    "    df = get_splits(text_df)\n",
    "    df = tidy_extraction(df)\n",
    "    df = get_window_df(text_df, df)\n",
    "    df.write_parquet('/tmp/extracted.parquet')\n",
    "    df = assume_type(df)\n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results: l.info(r)\n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r)\n",
    "\n",
    "if __name__=='__main__': main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a17add",
   "metadata": {
    "_cell_guid": "af30506a-e45e-4a3b-ba81-23dd154bde8d",
    "_uuid": "94fc4779-0b43-4058-ac64-f54a7ebd8a76",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-29T04:45:29.872022Z",
     "iopub.status.busy": "2025-08-29T04:45:29.871794Z",
     "iopub.status.idle": "2025-08-29T04:45:29.878451Z",
     "shell.execute_reply": "2025-08-29T04:45:29.877873Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015348,
     "end_time": "2025-08-29T04:45:29.879459",
     "exception": false,
     "start_time": "2025-08-29T04:45:29.864111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/llm_validate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/llm_validate.py\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "# ===============================\n",
    "# Few-shot Âº∑ÂåñÊ∏à„Åø„Éó„É≠„É≥„Éó„Éà + ÊñáËÑà„Éë„Çø„Éº„É≥ÊòéÁ§∫ + „Ç¢„ÇØ„Çª„ÉÉ„Ç∑„Éß„É≥‰æãËøΩÂä†\n",
    "# ===============================\n",
    "SYS_PROMPT_CLASSIFY_DOI = \"\"\"\n",
    "1. Priority Rules (highest ‚Üí lowest)\n",
    "\n",
    "1.1 Always classify as A (Data) if:\n",
    "DOI prefix matches a known data repository:\n",
    "\n",
    "Dryad: 10.5061\n",
    "Zenodo: 10.5281\n",
    "Figshare: 10.6084\n",
    "Mendeley Data: 10.24433/, 10.17632\n",
    "Dataverse: 10.7910/DVN\n",
    "OpenNeuro: 10.18112/openneuro.\n",
    "PANGAEA: 10.1594/PANGAEA.\n",
    "Neotoma Paleoecology: 10.21233\n",
    "ICPSR: 10.3886\n",
    "NOAA NCEI: 10.7289\n",
    "UK Data Service: 10.5255\n",
    "EMPIAR: 10.6019\n",
    "\n",
    "Non-DOI dataset accession prefixes:\n",
    "NCBI SRA / ENA: SRP, SRA, ERP, ERX\n",
    "BioProject: PRJNA, PRJEB, PRJDB, SAMN\n",
    "ProteomeXchange / PRIDE: PXD\n",
    "ArrayExpress / EMBL-EBI: E-MTAB, E-\n",
    "MetaboLights: MTBLS\n",
    "GEO Series: GSE\n",
    "GenBank: MN, NC_, CP, MT (context needed)\n",
    "EMDB: EMD-\n",
    "EMPIAR: EMPIAR-\n",
    "\n",
    "1.2 Context keywords trigger A (Data)\n",
    "If the context contains any of the following patterns ‚Üí classify as A:\n",
    "- hosted on\n",
    "- deposited at\n",
    "- available via\n",
    "- uploaded to\n",
    "- stored on\n",
    "- accessible via\n",
    "- provided by\n",
    "- deposited in\n",
    "- archived at\n",
    "- supplementary dataset / supporting dataset\n",
    "- experimental data / raw data\n",
    "\n",
    "2. Classify as B (Literature) if:\n",
    "DOI prefix belongs to a publisher (e.g., 10.1038, 10.1007, etc.).\n",
    "Context patterns ‚Üí classify as B:\n",
    "- published in\n",
    "- as described in\n",
    "- reported in\n",
    "- method details published in\n",
    "- supplementary material / supplementary information only\n",
    "\n",
    "3. Ambiguous cases\n",
    "No repository prefix and no clear context ‚Üí default to B.\n",
    "Rare accession formats ‚Üí rely on context keywords.\n",
    "\n",
    "4. Output\n",
    "Only output:\n",
    "A ‚Üí data repository / dataset\n",
    "B ‚Üí literature / non-data resource\n",
    "\n",
    "5. Few-shot examples („Ç¢„ÇØ„Çª„ÉÉ„Ç∑„Éß„É≥‰æãËøΩÂä†, ÊñáËÑà„Éë„Çø„Éº„É≥ÊòéÁ§∫, 25 ‰ª∂‰ª•‰∏ä)\n",
    "‚ÄúRaw images are hosted on Figshare (DOI 10.6084/m9.figshare.1234567).‚Äù ‚Üí A\n",
    "‚ÄúSequence reads deposited at BioProject accession PRJNA765432.‚Äù ‚Üí A\n",
    "‚ÄúMethod details published in J. Proteome Res. DOI: 10.1021/acs.jproteome.0c00845.‚Äù ‚Üí B\n",
    "‚ÄúAs described in Nature Methods (DOI 10.1038/s41592-020-0793-2).‚Äù ‚Üí B\n",
    "‚ÄúSee Supplementary Data available via Zenodo (10.5281/zenodo.987654).‚Äù ‚Üí A\n",
    "‚ÄúData uploaded to Dryad (10.5061/dryad.x1y2z3).‚Äù ‚Üí A\n",
    "‚ÄúReferenced paper reported in bioRxiv DOI 10.1101/2020.01.01.123456.‚Äù ‚Üí B\n",
    "‚ÄúMetabolomics data in MetaboLights MTBLS1234.‚Äù ‚Üí A\n",
    "‚ÄúThe MRI scans are deposited at OpenNeuro (DOI 10.18112/openneuro.ds000001.v1.0.0).‚Äù ‚Üí A\n",
    "‚ÄúProtein structure described in Science (DOI 10.1126/science.abc1234).‚Äù ‚Üí B\n",
    "‚ÄúMicrobiome raw data hosted on ArrayExpress E-MTAB-9876.‚Äù ‚Üí A\n",
    "‚ÄúRNA sequencing reads available via SRA SRP098765.‚Äù ‚Üí A\n",
    "‚ÄúSupplementary tables published in Nature Genetics (DOI 10.1038/ng.123456).‚Äù ‚Üí B\n",
    "‚ÄúData from neuroimaging study uploaded to OpenNeuro.‚Äù ‚Üí A\n",
    "‚ÄúProteomics dataset deposited at PRIDE PXD012345.‚Äù ‚Üí A\n",
    "‚ÄúSequencing metadata hosted on Zenodo.‚Äù ‚Üí A\n",
    "‚ÄúResults presented in conference proceedings (DOI 10.1109/ICML.2020.12345).‚Äù ‚Üí B\n",
    "‚ÄúGenBank accession MN1234567 contains raw sequences.‚Äù ‚Üí A\n",
    "‚ÄúCryo-EM map deposited at EMDB EMD-9876.‚Äù ‚Üí A\n",
    "‚ÄúClinical trial dataset uploaded to Dryad.‚Äù ‚Üí A\n",
    "‚ÄúExperimental raw data in MetaboLights MTBLS5678.‚Äù ‚Üí A\n",
    "‚ÄúSupplementary figures published in Science (DOI 10.1126/science.abc12345).‚Äù ‚Üí B\n",
    "‚ÄúRaw imaging files stored on Figshare.‚Äù ‚Üí A\n",
    "‚ÄúReferenced preprint DOI 10.1101/2021.01.01.123456.‚Äù ‚Üí B\n",
    "‚ÄúData portal provides access to large-scale RNA-seq datasets.‚Äù ‚Üí A\n",
    "‚ÄúStudy reported in Nature Communications (DOI 10.1038/s41467-020-12345).‚Äù ‚Üí B\n",
    "‚ÄúSAMN123456 metadata contains raw sequences from microbiome study.‚Äù ‚Üí A\n",
    "\"\"\".strip()\n",
    "\n",
    "# ===============================\n",
    "# „Éá„Éº„Çø„Éï„É¨„Éº„É†ÊßãÁØâ\n",
    "# ===============================\n",
    "def build_df():\n",
    "    df = pl.read_parquet('/tmp/extracted.parquet')\n",
    "    df.filter(~is_doi_link('dataset_id')).select('article_id', 'dataset_id').write_csv('/tmp/accid_sub.csv')\n",
    "    return df.filter(is_doi_link('dataset_id'))\n",
    "\n",
    "# ===============================\n",
    "# „Éó„É≠„É≥„Éó„Éà‰ΩúÊàê\n",
    "# ===============================\n",
    "def build_prompt(tokenizer, df):\n",
    "    prompts = []\n",
    "    for doi, text in df.select('dataset_id', 'window').rows():\n",
    "        messages = [{'role':'system','content': SYS_PROMPT_CLASSIFY_DOI},\n",
    "                    {'role':'user', 'content': text}]\n",
    "        prompts.append(tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False))\n",
    "    return df.with_columns(pl.Series('prompt', prompts))\n",
    "\n",
    "# ===============================\n",
    "# „É°„Ç§„É≥Âá¶ÁêÜ\n",
    "# ===============================\n",
    "if __name__=='__main__':\n",
    "    os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "    import vllm\n",
    "    from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "\n",
    "    model_path = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "    llm = vllm.LLM(\n",
    "        model_path,\n",
    "        quantization='awq',\n",
    "        tensor_parallel_size=2,\n",
    "        gpu_memory_utilization=0.9,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=2048,\n",
    "        disable_log_stats=True,\n",
    "        disable_custom_all_reduce=True,\n",
    "        enable_prefix_caching=True,\n",
    "        task='generate'\n",
    "    )\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    df = build_df()\n",
    "    df = build_prompt(tokenizer, df)\n",
    "    prompts = df['prompt'].to_list()\n",
    "\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\"])\n",
    "    outputs = llm.generate(\n",
    "        prompts,\n",
    "        vllm.SamplingParams(\n",
    "            seed=777,\n",
    "            temperature=0,\n",
    "            skip_special_tokens=True,\n",
    "            max_tokens=1,\n",
    "            logits_processors=[mclp],\n",
    "            logprobs=len(mclp.choices)\n",
    "        ),\n",
    "        use_tqdm=True\n",
    "    )\n",
    "\n",
    "    logprobs = [\n",
    "        {lp.decoded_token: lp.logprob for lp in list(lps)}\n",
    "        for lps in [output.outputs[0].logprobs[0].values() for output in outputs]\n",
    "    ]\n",
    "    choices = [max(d, key=d.get) for d in logprobs]\n",
    "    types = {'A': True, 'B': False}\n",
    "    choices = [types[c] for c in choices]\n",
    "\n",
    "    df = df.with_columns(pl.Series('type', choices))\n",
    "    df.filter(pl.col('type')).select('article_id', 'dataset_id').write_csv('/tmp/doi_sub.csv')\n",
    "    df = pl.concat([pl.read_csv('/tmp/doi_sub.csv'), pl.read_csv('/tmp/accid_sub.csv')])\n",
    "    df = assume_type(df)\n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results:\n",
    "            l.info(r)\n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results:\n",
    "            l.info(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33a4de7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T04:45:29.894761Z",
     "iopub.status.busy": "2025-08-29T04:45:29.894407Z",
     "iopub.status.idle": "2025-08-29T04:45:29.899456Z",
     "shell.execute_reply": "2025-08-29T04:45:29.898824Z"
    },
    "papermill": {
     "duration": 0.013766,
     "end_time": "2025-08-29T04:45:29.900539",
     "exception": false,
     "start_time": "2025-08-29T04:45:29.886773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/post_filter.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/post_filter.py\n",
    "import polars as pl\n",
    "from helpers import *\n",
    "\n",
    "\"\"\"\n",
    "Fourth essence: Post-filter to cut FP DOIs that look like literature.\n",
    "- Read /kaggle/working/submission.csv (output of llm_validate.py)\n",
    "- Join with /tmp/extracted.parquet to get context window\n",
    "- Drop DOI rows that (1) start with typical publisher prefixes AND (2) have no data-ish words nearby\n",
    "- Keep accessions untouched\n",
    "\"\"\"\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "PAPER_PREFIXES = [\n",
    "    \"10.5061\",\"10.5281\",\"10.17632\",\"10.1594\",\"10.15468\",\"10.17882\",\"10.7937\",\"10.7910\",\"10.6073\",\n",
    "    \"10.3886\",\"10.3334\",\"10.4121\",\"10.5066\",\"10.5067\",\"10.18150\",\"10.25377\",\"10.25387\",\"10.23642\",\"10.24381\",\"10.22033\"\n",
    "]\n",
    "\n",
    "CONTEXT_RE = r\"(?i)\\b(data(?:set)?|repository|archive|deposited|available|supplementary|raw(?:\\s+data)?|uploaded|hosted|stored|accession)\\b\"\n",
    "\n",
    "def remove_extra_digit(df: pl.DataFrame, column: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows where the value in `column` is just the same DOI with one extra digit at the end.\n",
    "    Keeps all other columns.\n",
    "    \"\"\"\n",
    "    items_set = set(df[column].to_list())\n",
    "\n",
    "    def keep_row(value):\n",
    "        if (value[-1].isdigit() and value[:-1] in items_set) or \\\n",
    "           (len(value) > 2 and value[-2:].isdigit() and value[:-2] in items_set):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    return df.filter(pl.col(column).map_elements(keep_row, return_dtype=pl.Boolean))\n",
    "def is_paper_prefix(col: str = \"dataset_id\") -> pl.Expr:\n",
    "    expr = pl.lit(False)\n",
    "    for p in PAPER_PREFIXES:\n",
    "        expr = expr | pl.col(col).str.starts_with(f\"{DOI_LINK}{p}\")\n",
    "    return expr\n",
    "\n",
    "def main():\n",
    "    sub = pl.read_csv(\"/kaggle/working/submission.csv\")\n",
    "\n",
    "    # Normalize columns: drop row_id if present so concat widths match\n",
    "    if \"row_id\" in sub.columns:\n",
    "        sub = sub.drop(\"row_id\")\n",
    "\n",
    "    # Context windows\n",
    "    win = pl.read_parquet(\"/tmp/extracted.parquet\").select(\"article_id\", \"dataset_id\", \"window\")\n",
    "\n",
    "    # DOI & ACC split\n",
    "    doi_rows = sub.filter(is_doi_link(\"dataset_id\")).join(win, on=[\"article_id\", \"dataset_id\"], how=\"left\")\n",
    "    acc_rows = sub.filter(~is_doi_link(\"dataset_id\"))\n",
    "\n",
    "    keep_mask = (\n",
    "        (~is_paper_prefix(\"dataset_id\"))  # not a known paper prefix\n",
    "        | doi_rows[\"window\"].fill_null(\"\").str.contains(CONTEXT_RE)\n",
    "    )\n",
    "\n",
    "    kept_doi = doi_rows.filter(keep_mask).select(\"article_id\", \"dataset_id\", \"type\")\n",
    "    doi_df = remove_extra_digit(kept_doi, \"dataset_id\")\n",
    "    final = pl.concat([doi_df, acc_rows.select(\"article_id\", \"dataset_id\", \"type\")])\n",
    "\n",
    "    # Re-eval & save\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        for r in evaluate(final): l.info(r)\n",
    "        for r in evaluate(final, on=[\"article_id\", \"dataset_id\", \"type\"]): l.info(r)\n",
    "\n",
    "    final.with_row_index(\"row_id\").write_csv(\"/kaggle/working/submission.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0cc80f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T04:45:29.915921Z",
     "iopub.status.busy": "2025-08-29T04:45:29.915600Z",
     "iopub.status.idle": "2025-08-29T04:45:29.922137Z",
     "shell.execute_reply": "2025-08-29T04:45:29.921605Z"
    },
    "papermill": {
     "duration": 0.015392,
     "end_time": "2025-08-29T04:45:29.923110",
     "exception": false,
     "start_time": "2025-08-29T04:45:29.907718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/post_validate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/post_validate.py\n",
    "\n",
    "from helpers import *\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "\n",
    "PROMPT_CLASSIFY_CITATION_TYPE = '''\n",
    "# Role & Task\n",
    "You are an expert data citation analyst. Your task is to classify a given citation from a scientific paper into one of two categories: **A** (Data) or **B** (Not Data). Base your decision strictly on the provided abstract and the context of the citation.\n",
    "\n",
    "## Instructions\n",
    "1.  **Read the provided abstract** to understand the research context.\n",
    "2.  **Analyze the citation context** for key linguistic cues.\n",
    "3.  **Classify the citation** as either **A** or **B** based on the definitions below.\n",
    "4.  **Output only a single letter: A or B.** Do not output any other text, explanation, or formatting.\n",
    "\n",
    "## Category Definitions\n",
    "\n",
    "### **Category A: DATA**\n",
    "The citation points to a dataset. This includes:\n",
    "*   **Primary Data:** Raw or processed data that the current study's authors collected, generated, or created.\n",
    "*   **Secondary Data:** Data that was originally produced by other researchers but is being *used as a dataset* in the current study.\n",
    "*   **Key Phrases:** \"data are available at\", \"we collected\", \"we measured\", \"data were obtained from\", \"dataset\", \"downloaded from\", \"deposited in\", repository names (e.g., GenBank, Zenodo, Figshare, TCIA).\n",
    "\n",
    "### **Category B: NOT DATA**\n",
    "The citation points to a traditional scholarly publication or other non-data resource. This includes:\n",
    "*   Journal articles, books, conference proceedings, preprints, protocols, methods papers.\n",
    "*   **Key Phrases:** \"as described in\", \"according to\", \"previous study\", \"et al.\", \"paper\", \"article\", \"methodology\", \"was used for analysis\" (without indicating data access).\n",
    "*   Citations that provide background context or methodological description but do not serve as the source of the data used in the analysis.\n",
    "\n",
    "## Input Format\n",
    "You will be provided with the following three pieces of information:\n",
    "Paper Abstract: {abstract}\n",
    "Citation: {dataset_id}\n",
    "Citation Context: {context}\n",
    "\n",
    "## Critical Thinking Guidelines\n",
    "*   A DOI or URL can point to either data (A) or a paper (B). The context determines the classification.\n",
    "*   If the citation is used to describe the *source* of the data for the current study's analysis, it is likely **A**.\n",
    "*   If the citation is used to provide background, justify a method, or compare results, it is likely **B** (a reference to another paper).\n",
    "*   When in doubt, rely on the linguistic cues in the \"Citation Context\".\n",
    "\n",
    "## Examples for Pattern Recognition\n",
    "\n",
    "**Example 1 (Classify as A):**\n",
    "*   Context: \"Three out of four cohorts used in this study can be found on The Cancer Imaging Archive (TCIA)24: Canadian benchmark dataset23: https://doi.org/10.7937/K9/TCIA.2017.8oje5q00.\"\n",
    "*   **Reasoning:** The text states cohorts are \"used in this study\" and provides direct repository links. This is a clear case of citing external data for use.\n",
    "*   **Output:** A\n",
    "\n",
    "**Example 2 (Classify as B):**\n",
    "*   Context: \"data presented here are available at the SEANOE dataportal: https://doi.org/10.17882/94052 (ZooScan dataset Grandremy et al. 2023c)\"\n",
    "*   **Reasoning:** The phrase \"data presented here\" indicates this is the authors' own data being deposited, not a citation to an external source they are using. The \"(Author et al. Year)\" format is a classic literature citation style.\n",
    "*   **Output:** B\n",
    "\n",
    "**Example 3 (Classify as A):**\n",
    "*   Context: \"GBIF occurrence data: Vulpes vulpes: https://doi.org/10.15468/dl.wgtneb (28 May 2021).\"\n",
    "*   **Reasoning:** Explicitly names the data source (GBIF) and provides a direct access link/DOI for the specific dataset used.\n",
    "*   **Output:** A\n",
    "\n",
    "**Example 4 (Classify as B):**\n",
    "*   Context: \"North American soil NCBI SRA SRP035367 Smith & Peay [36] ITS2-Soil\"\n",
    "*   **Reasoning:** While it mentions a data repository ID (SRP035367), it couples it with a standard literature citation \"[36]\". The context suggests it is referencing the *paper* by Smith & Peay that describes the data, not directly citing the dataset itself for use.\n",
    "*   **Output:** B\n",
    "\n",
    "## Ready for Input\n",
    "Begin your analysis. Remember: Output only **A** or **B**.\n",
    "'''\n",
    "\n",
    "def get_context_window(text: str, substring: str, window: int = 600) -> str:\n",
    "    idx = text.find(substring)\n",
    "    if idx == -1:\n",
    "        return \"no context\", \"no abstraction\"\n",
    "    start = max(idx - window, 0)\n",
    "    end = min(idx + len(substring) + window, len(text))\n",
    "    return text[start:end] , text[:1000]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_context_win(tokenizer,df):\n",
    "    text_df = pl.read_parquet('/tmp/context_data.parquet')\n",
    "    # print(text_df)\n",
    "    df = df.join(text_df, on=[\"article_id\",\"dataset_id\"], how=\"inner\")\n",
    "    df = df.drop(\"type\")\n",
    "    print(df)\n",
    "\n",
    "    prompts = []\n",
    "    \n",
    "    for article_id,dataset_id,text,match in df.select([\"article_id\",\"dataset_id\",\"text\",'match']).rows():\n",
    "\n",
    "        context, abstract = get_context_window(text,match)\n",
    "        user_content = f\"\"\"\n",
    "        Paper Abstract: {abstract}\n",
    "        \n",
    "        Citation: {dataset_id}\n",
    "\n",
    "        \n",
    "        Citation Context: {context}\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": PROMPT_CLASSIFY_CITATION_TYPE},\n",
    "            {\"role\": \"user\", \"content\": user_content.strip()}\n",
    "        ]\n",
    "        prompts.append(\n",
    "            tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "        )\n",
    "        \n",
    "    return df.with_columns(pl.Series(\"prompt\", prompts))\n",
    "\n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "    MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "    import vllm\n",
    "    from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "\n",
    "    llm = vllm.LLM(\n",
    "        MODEL_PATH,\n",
    "        quantization='awq',\n",
    "        tensor_parallel_size=2,\n",
    "        gpu_memory_utilization=0.9,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=16384,\n",
    "        disable_log_stats=True, \n",
    "        disable_custom_all_reduce=True,\n",
    "        enable_prefix_caching=True,\n",
    "        task='generate')\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "\n",
    "    df=pl.read_csv(\"/kaggle/working/submission.csv\")\n",
    "    \n",
    "    if \"row_id\" in df.columns:\n",
    "        df = df.drop(\"row_id\")\n",
    "\n",
    "    # print(df)\n",
    "\n",
    "    doi_df = df.filter(is_doi_link(\"dataset_id\"))\n",
    "    acc_df = df.filter(~is_doi_link(\"dataset_id\"))\n",
    "\n",
    "    # print(doi_df)\n",
    "\n",
    "    df = find_context_win(tokenizer,doi_df)\n",
    "\n",
    "    \n",
    "    \n",
    "    prompts = df['prompt'].to_list()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\",\"C\"])\n",
    "    outputs = llm.generate(prompts, vllm.SamplingParams(seed=777, temperature=0.7, skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=len(mclp.choices)), use_tqdm=True)\n",
    "    logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n",
    "    choices = [max(d, key=d.get) for d in logprobs]\n",
    "    types = {'A': True, 'B': False}\n",
    "    choices = [types[c] for c in choices]\n",
    "    df = df.with_columns(pl.Series('type', choices))\n",
    "    df.filter(pl.col('type')).select('article_id', 'dataset_id').write_csv('/tmp/doi_sub.csv')\n",
    "    df = pl.concat([pl.read_csv('/tmp/doi_sub.csv'), pl.read_csv('/tmp/accid_sub.csv')])\n",
    "    df = assume_type(df)\n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    # print(df)\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results: l.info(r) \n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        del llm, tokenizer\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    import gc, torch\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7054e12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T04:45:29.938307Z",
     "iopub.status.busy": "2025-08-29T04:45:29.938104Z",
     "iopub.status.idle": "2025-08-29T04:45:29.944348Z",
     "shell.execute_reply": "2025-08-29T04:45:29.943715Z"
    },
    "papermill": {
     "duration": 0.015142,
     "end_time": "2025-08-29T04:45:29.945436",
     "exception": false,
     "start_time": "2025-08-29T04:45:29.930294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/predict.py\n",
    "\n",
    "from helpers import *\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "\n",
    "l = get_logger()\n",
    "\n",
    "\n",
    "PROMPT_CLASSIFY_CITATION_TYPE = '''\n",
    "# Role & Task\n",
    "You are an expert data citation analyst. Your task is to classify a given citation from a scientific paper into one of two categories based on the context: **A (Primary Data)** or **B (Secondary Data)**.\n",
    "\n",
    "## Instructions\n",
    "1.  **Read the provided abstract** to understand the research context.\n",
    "2.  **Analyze the citation context** for key linguistic cues.\n",
    "3.  **Classify the citation** as either **A** or **B** based on the definitions below.\n",
    "4.  **Output only a single letter: A or B.** Do not output any other text, explanation, or formatting.\n",
    "\n",
    "## Category Definitions\n",
    "\n",
    "### **Category A: PRIMARY DATA**\n",
    "The data was generated, collected, or created by the **authors of the current study**. This is *their* data.\n",
    "*   **Key Phrases:** \"we collected\", \"we generated\", \"our data\", \"data are available at [URL/DOI]\", \"data have been deposited\", \"this study presents\", \"supplementary data\".\n",
    "\n",
    "### **Category B: SECONDARY DATA**\n",
    "The data was produced by **other researchers** or external sources and is being reused or analyzed by the current study's authors.\n",
    "*   **Key Phrases:** \"data were obtained from\", \"publicly available data\", \"previously published data\", \"retrieved from\", \"downloaded from\", \"[Dataset Name] dataset\", \"database\", citing a specific external source.\n",
    "\n",
    "## Input Format\n",
    "You will be provided with the following three pieces of information:\n",
    "Paper Abstract: {abstract}\n",
    "Citation: {dataset_id}\n",
    "Citation Context: {context}\n",
    "\n",
    "\n",
    "## Decision Framework\n",
    "Answer these questions based on the **Citation Context**:\n",
    "\n",
    "1.  **Who is the source of the data?**\n",
    "    *   If the context implies the **authors themselves** are the source (e.g., \"we,\" \"our\"), classify as **A**.\n",
    "    *   If the context names an **external source** (e.g., a repository, another study, a database), classify as **B**.\n",
    "\n",
    "2.  **What is the action being described?**\n",
    "    *   **A (Primary)** actions: *depositing, making available, presenting* their own data.\n",
    "    *   **B (Secondary)** actions: *using, obtaining, accessing, downloading, analyzing* existing data from elsewhere.\n",
    "\n",
    "## Examples for Pattern Recognition\n",
    "\n",
    "**Example 1 (Classify as B):**\n",
    "*   Context: \"Three out of four cohorts **used in this study** can be found on The Cancer Imaging Archive (TCIA)24: Canadian benchmark dataset23: https://doi.org/10.7937/K9/TCIA.2017.8oje5q00.\"\n",
    "*   **Reasoning:** The authors are describing external datasets they **used** (a Secondary action). The source is TCIA, not themselves.\n",
    "*   **Output:** B\n",
    "\n",
    "**Example 2 (Classify as A):**\n",
    "*   Context: \"Additional research data **supporting this publication are available** at 10.25377/sussex.21184705.\"\n",
    "*   **Reasoning:** The authors are stating the availability of data that **supports their own publication**. The source is implied to be themselves.\n",
    "*   **Output:** A\n",
    "\n",
    "**Example 3 (Classify as B):**\n",
    "*   Context: \"GBIF occurrence data: Vulpes vulpes: https://doi.org/10.15468/dl.wgtneb (28 May 2021).\"\n",
    "*   **Reasoning:** The data is explicitly sourced from an external repository (GBIF). The authors are referring to data they reused.\n",
    "*   **Output:** B\n",
    "\n",
    "**Example 4 (Classify as A):**\n",
    "*   Context: \"Data referring to Barbieux et al. (2017; https://doi.org/10.17882/49388) are freely available on SEANOE.\"\n",
    "*   **Reasoning:** This is a tricky case. The citation format \"(Author et al. Year)\" suggests a literature reference. However, the phrase \"Data referring to\" and the direct data DOI indicate the authors are citing **their own previously published dataset** (from a 2017 paper) that is now available. This is their Primary data.\n",
    "*   **Output:** A\n",
    "\n",
    "## Ready for Input\n",
    "Begin your analysis. Remember: Output only **A** or **B**.\n",
    "\n",
    "'''\n",
    "\n",
    "def get_context_window(text: str, substring: str, window: int = 600) -> str:\n",
    "    idx = text.find(substring)\n",
    "    if idx == -1:\n",
    "        return \"no context\", \"no abstraction\"\n",
    "    start = max(idx - window, 0)\n",
    "    end = min(idx + len(substring) + window, len(text))\n",
    "    return text[start:end] , text[:1000]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_context_win(tokenizer,df):\n",
    "    text_df = pl.read_parquet('/tmp/context_data.parquet')\n",
    "    # print(text_df)\n",
    "    df = df.join(text_df, on=[\"article_id\",\"dataset_id\"], how=\"inner\")\n",
    "    df = df.drop(\"type\")\n",
    "    print(df)\n",
    "\n",
    "    prompts = []\n",
    "    \n",
    "    for article_id,dataset_id,text,match in df.select([\"article_id\",\"dataset_id\",\"text\",'match']).rows():\n",
    "\n",
    "        context, abstract = get_context_window(text,match)\n",
    "        user_content = f\"\"\"\n",
    "        Paper Abstract: {abstract}\n",
    "        \n",
    "        Citation: {dataset_id}\n",
    "\n",
    "        \n",
    "        Citation Context: {context}\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": PROMPT_CLASSIFY_CITATION_TYPE},\n",
    "            {\"role\": \"user\", \"content\": user_content.strip()}\n",
    "        ]\n",
    "        prompts.append(\n",
    "            tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "        )\n",
    "        \n",
    "    return df.with_columns(pl.Series(\"prompt\", prompts))\n",
    "\n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "    MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1\"\n",
    "    import vllm\n",
    "    from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "\n",
    "    llm = vllm.LLM(\n",
    "        MODEL_PATH,\n",
    "        quantization='awq',\n",
    "        tensor_parallel_size=2,\n",
    "        gpu_memory_utilization=0.9,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=16384,\n",
    "        disable_log_stats=True, \n",
    "        disable_custom_all_reduce=True,\n",
    "        enable_prefix_caching=True,\n",
    "        task='generate')\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "\n",
    "    df=pl.read_csv(\"/kaggle/working/submission.csv\")\n",
    "    \n",
    "    if \"row_id\" in df.columns:\n",
    "        df = df.drop(\"row_id\")\n",
    "\n",
    "\n",
    "    doi_df = df.filter(is_doi_link(\"dataset_id\"))\n",
    "    acc_df = df.filter(~is_doi_link(\"dataset_id\"))\n",
    "\n",
    "\n",
    "\n",
    "    df = find_context_win(tokenizer,doi_df)\n",
    "\n",
    "    \n",
    "    \n",
    "    prompts = df['prompt'].to_list()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[\"A\", \"B\"])\n",
    "    outputs = llm.generate(prompts, vllm.SamplingParams(seed=777, temperature=0.8, skip_special_tokens=True, max_tokens=1, logits_processors=[mclp], logprobs=len(mclp.choices)), use_tqdm=True)\n",
    "    logprobs = [{lp.decoded_token: lp.logprob for lp in list(lps)} for lps in [output.outputs[0].logprobs[0].values() for output in outputs]]\n",
    "    choices = [max(d, key=d.get) for d in logprobs]\n",
    "    types = {'A':'Primary', 'B':'Secondary'}\n",
    "    choices = [types[c] for c in choices]\n",
    "\n",
    "\n",
    "    \n",
    "    df = df.with_columns(pl.Series('type', choices))\n",
    "    df.select('article_id', 'dataset_id','type').write_csv('/tmp/doi_sub.csv')\n",
    "\n",
    "    acc_df = assume_type(acc_df)\n",
    "    acc_df.select('article_id','dataset_id','type').write_csv(\"/tmp/accid_sub.csv\")\n",
    "    df = pl.concat([pl.read_csv('/tmp/doi_sub.csv'), pl.read_csv('/tmp/accid_sub.csv')])\n",
    "    \n",
    "    df.select(['article_id', 'dataset_id', 'type']).with_row_index(name='row_id').write_csv('/kaggle/working/submission.csv')\n",
    "    # print(df)\n",
    "    if not IS_KAGGLE_SUBMISSION:\n",
    "        results = evaluate(df)\n",
    "        for r in results: l.info(r) \n",
    "        results = evaluate(df, on=['article_id', 'dataset_id', 'type'])\n",
    "        for r in results: l.info(r)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        del llm, tokenizer\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    import gc, torch\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4cb20ee",
   "metadata": {
    "_cell_guid": "c6a12705-737a-4b21-9bf2-125b3d1ab724",
    "_kg_hide-output": true,
    "_uuid": "bc8e0d68-3097-4ce9-99a1-536921913550",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-08-29T04:45:29.962380Z",
     "iopub.status.busy": "2025-08-29T04:45:29.961940Z",
     "iopub.status.idle": "2025-08-29T04:59:36.610776Z",
     "shell.execute_reply": "2025-08-29T04:59:36.609945Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 846.658841,
     "end_time": "2025-08-29T04:59:36.612128",
     "exception": false,
     "start_time": "2025-08-29T04:45:29.953287",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp\n",
      "PDF‚ÜíTXT:  13%|‚ñà‚ñà‚ñà‚ñà‚ñè                            | 67/524 [00:11<01:55,  3.96it/s]MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\r\n",
      "\r\n",
      "PDF‚ÜíTXT: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 524/524 [01:39<00:00,  5.29it/s]\r\n",
      "XML‚ÜíTXT: 400it [00:05, 78.62it/s]\r\n",
      "Done parsing to text.\r\n",
      "INFO 2025-08-29 04:47:20  [check_parse.py:31 - main()] pymupdf misses: 26 dataset_ids\r\n",
      "INFO 2025-08-29 04:47:33  [getid.py:269 - main()] all - f1: 0.5543 [536/679/183]\r\n",
      "INFO 2025-08-29 04:47:33  [getid.py:269 - main()] doi - f1: 0.4316 [172/300/153]\r\n",
      "INFO 2025-08-29 04:47:33  [getid.py:269 - main()] acc - f1: 0.6403 [364/379/30]\r\n",
      "INFO 2025-08-29 04:47:33  [getid.py:271 - main()] all - f1: 0.4685 [453/762/266]\r\n",
      "INFO 2025-08-29 04:47:33  [getid.py:271 - main()] doi - f1: 0.3363 [134/338/191]\r\n",
      "INFO 2025-08-29 04:47:33  [getid.py:271 - main()] acc - f1: 0.5611 [319/424/75]\r\n",
      "INFO 08-29 04:47:57 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "WARNING 08-29 04:48:15 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 08-29 04:48:15 [config.py:1770] Defaulting to use mp for distributed inference\r\n",
      "WARNING 08-29 04:48:15 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 08-29 04:48:15 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \r\n",
      "WARNING 08-29 04:48:16 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=155)\u001b[0;0m INFO 08-29 04:48:16 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\r\n",
      "INFO 08-29 04:48:16 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 08-29 04:48:16 [cuda.py:289] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=155)\u001b[0;0m INFO 08-29 04:48:16 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=155)\u001b[0;0m INFO 08-29 04:48:16 [cuda.py:289] Using XFormers backend.\r\n",
      "[W829 04:48:27.316107537 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W829 04:48:28.676740637 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W829 04:48:37.323757842 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W829 04:48:48.334218903 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=155)\u001b[0;0m INFO 08-29 04:48:48 [utils.py:1055] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=155)\u001b[0;0m INFO 08-29 04:48:48 [pynccl.py:69] vLLM is using nccl==2.21.5\r\n",
      "INFO 08-29 04:48:48 [utils.py:1055] Found nccl from library libnccl.so.2\r\n",
      "INFO 08-29 04:48:48 [pynccl.py:69] vLLM is using nccl==2.21.5\r\n",
      "INFO 08-29 04:48:48 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_91387fcc'), local_subscribe_addr='ipc:///tmp/5a647ef8-e0c4-4e5d-a1a7-b17674aecf55', remote_subscribe_addr=None, remote_addr_ipv6=False)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=155)\u001b[0;0m INFO 08-29 04:48:48 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\r\n",
      "INFO 08-29 04:48:48 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 08-29 04:48:48 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=155)\u001b[0;0m INFO 08-29 04:48:48 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:30<02:02, 30.63s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [01:05<01:40, 33.35s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [01:48<01:15, 37.66s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [02:32<00:40, 40.14s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [03:15<00:00, 41.23s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [03:15<00:00, 39.15s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=155)\u001b[0;0m INFO 08-29 04:52:04 [loader.py:458] Loading weights took 195.87 seconds\r\n",
      "INFO 08-29 04:52:05 [loader.py:458] Loading weights took 196.13 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=155)\u001b[0;0m INFO 08-29 04:52:05 [model_runner.py:1140] Model loading took 9.0935 GiB and 196.352576 seconds\r\n",
      "INFO 08-29 04:52:05 [model_runner.py:1140] Model loading took 9.0935 GiB and 196.610213 seconds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=155)\u001b[0;0m INFO 08-29 04:52:20 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 0.44GiB; the rest of the memory reserved for KV Cache is 3.63GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 08-29 04:52:20 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 1.41GiB; the rest of the memory reserved for KV Cache is 2.66GiB.\r\n",
      "INFO 08-29 04:52:20 [executor_base.py:112] # cuda blocks: 1363, # CPU blocks: 2048\r\n",
      "INFO 08-29 04:52:20 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 10.65x\r\n",
      "INFO 08-29 04:52:24 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 19.11 seconds\r\n",
      "Processed prompts: 100%|‚ñà| 472/472 [01:56<00:00,  4.05it/s, est. speed input: 54\r\n",
      "INFO 2025-08-29 04:54:23  [llm_validate.py:182 - <module>()] all - f1: 0.6158 [517/443/202]\r\n",
      "INFO 2025-08-29 04:54:23  [llm_validate.py:182 - <module>()] doi - f1: 0.5646 [153/64/172]\r\n",
      "INFO 2025-08-29 04:54:23  [llm_validate.py:182 - <module>()] acc - f1: 0.6403 [364/379/30]\r\n",
      "INFO 2025-08-29 04:54:23  [llm_validate.py:185 - <module>()] all - f1: 0.5265 [442/518/277]\r\n",
      "INFO 2025-08-29 04:54:23  [llm_validate.py:185 - <module>()] doi - f1: 0.4539 [123/94/202]\r\n",
      "INFO 2025-08-29 04:54:23  [llm_validate.py:185 - <module>()] acc - f1: 0.5611 [319/424/75]\r\n",
      "INFO 08-29 04:54:25 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\r\n",
      "[rank0]:[W829 04:54:25.318125401 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Exception ignored in: <Finalize object, dead>\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 227, in __call__\r\n",
      "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 206, in _finalize_close\r\n",
      "TypeError: 'NoneType' object is not callable\r\n",
      "INFO 2025-08-29 04:54:31  [post_filter.py:66 - main()] all - f1: 0.6134 [507/427/212]\r\n",
      "INFO 2025-08-29 04:54:31  [post_filter.py:66 - main()] doi - f1: 0.5543 [143/48/182]\r\n",
      "INFO 2025-08-29 04:54:31  [post_filter.py:66 - main()] acc - f1: 0.6403 [364/379/30]\r\n",
      "INFO 2025-08-29 04:54:31  [post_filter.py:67 - main()] all - f1: 0.5287 [437/497/282]\r\n",
      "INFO 2025-08-29 04:54:31  [post_filter.py:67 - main()] doi - f1: 0.4574 [118/73/207]\r\n",
      "INFO 2025-08-29 04:54:31  [post_filter.py:67 - main()] acc - f1: 0.5611 [319/424/75]\r\n",
      "INFO 08-29 04:54:38 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "WARNING 08-29 04:54:52 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 08-29 04:54:52 [config.py:1770] Defaulting to use mp for distributed inference\r\n",
      "WARNING 08-29 04:54:52 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 08-29 04:54:52 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \r\n",
      "WARNING 08-29 04:54:52 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=343)\u001b[0;0m INFO 08-29 04:54:52 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\r\n",
      "INFO 08-29 04:54:53 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 08-29 04:54:53 [cuda.py:289] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=343)\u001b[0;0m INFO 08-29 04:54:53 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=343)\u001b[0;0m INFO 08-29 04:54:53 [cuda.py:289] Using XFormers backend.\r\n",
      "[W829 04:55:04.378898484 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W829 04:55:04.732230613 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W829 04:55:14.389486230 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W829 04:55:24.399873175 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 08-29 04:55:24 [utils.py:1055] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=343)\u001b[0;0m INFO 08-29 04:55:24 [utils.py:1055] Found nccl from library libnccl.so.2\r\n",
      "INFO 08-29 04:55:24 [pynccl.py:69] vLLM is using nccl==2.21.5\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=343)\u001b[0;0m INFO 08-29 04:55:24 [pynccl.py:69] vLLM is using nccl==2.21.5\r\n",
      "INFO 08-29 04:55:24 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_b8ebaff4'), local_subscribe_addr='ipc:///tmp/027941d7-3704-4a22-9cb8-55ebfef7d390', remote_subscribe_addr=None, remote_addr_ipv6=False)\r\n",
      "INFO 08-29 04:55:24 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=343)\u001b[0;0m INFO 08-29 04:55:24 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\r\n",
      "INFO 08-29 04:55:24 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=343)\u001b[0;0m INFO 08-29 04:55:24 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:14<00:58, 14.64s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:28<00:42, 14.17s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:40<00:26, 13.18s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:44<00:09,  9.61s/it]\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=343)\u001b[0;0m INFO 08-29 04:56:10 [loader.py:458] Loading weights took 46.25 seconds\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:46<00:00,  6.83s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:46<00:00,  9.30s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=343)\u001b[0;0m INFO 08-29 04:56:11 [model_runner.py:1140] Model loading took 9.0935 GiB and 46.548454 seconds\r\n",
      "INFO 08-29 04:56:11 [loader.py:458] Loading weights took 46.69 seconds\r\n",
      "INFO 08-29 04:56:11 [model_runner.py:1140] Model loading took 9.0935 GiB and 46.995865 seconds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=343)\u001b[0;0m INFO 08-29 04:56:47 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 2.06GiB; the rest of the memory reserved for KV Cache is 2.01GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 08-29 04:56:47 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 2.06GiB; the rest of the memory reserved for KV Cache is 2.01GiB.\r\n",
      "INFO 08-29 04:56:47 [executor_base.py:112] # cuda blocks: 1031, # CPU blocks: 2048\r\n",
      "INFO 08-29 04:56:47 [executor_base.py:117] Maximum concurrency for 16384 tokens per request: 1.01x\r\n",
      "INFO 08-29 04:56:52 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 40.32 seconds\r\n",
      "[rank0]: Traceback (most recent call last):\r\n",
      "[rank0]:   File \"/tmp/src/post_validate.py\", line 149, in <module>\r\n",
      "[rank0]:     df = find_context_win(tokenizer,doi_df)\r\n",
      "[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/tmp/src/post_validate.py\", line 84, in find_context_win\r\n",
      "[rank0]:     text_df = pl.read_parquet('/tmp/context_data.parquet')\r\n",
      "[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/polars/_utils/deprecation.py\", line 92, in wrapper\r\n",
      "[rank0]:     return function(*args, **kwargs)\r\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/polars/_utils/deprecation.py\", line 92, in wrapper\r\n",
      "[rank0]:     return function(*args, **kwargs)\r\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/polars/io/parquet/functions.py\", line 241, in read_parquet\r\n",
      "[rank0]:     return lf.collect()\r\n",
      "[rank0]:            ^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/polars/lazyframe/frame.py\", line 2056, in collect\r\n",
      "[rank0]:     return wrap_df(ldf.collect(callback))\r\n",
      "[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]: FileNotFoundError: No such file or directory (os error 2): /tmp/context_data.parquet\r\n",
      "ERROR 08-29 04:56:53 [multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 343 died, exit code: -15\r\n",
      "INFO 08-29 04:56:53 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\r\n",
      "[rank0]:[W829 04:56:55.341343919 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Exception ignored in: <Finalize object, dead>\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 227, in __call__\r\n",
      "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 206, in _finalize_close\r\n",
      "TypeError: 'NoneType' object is not callable\r\n",
      "INFO 08-29 04:57:05 [__init__.py:239] Automatically detected platform cuda.\r\n",
      "WARNING 08-29 04:57:19 [config.py:830] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "INFO 08-29 04:57:19 [config.py:1770] Defaulting to use mp for distributed inference\r\n",
      "WARNING 08-29 04:57:19 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 08-29 04:57:19 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/32b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \r\n",
      "WARNING 08-29 04:57:19 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=467)\u001b[0;0m INFO 08-29 04:57:19 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks\r\n",
      "INFO 08-29 04:57:19 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 08-29 04:57:19 [cuda.py:289] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=467)\u001b[0;0m INFO 08-29 04:57:19 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=467)\u001b[0;0m INFO 08-29 04:57:19 [cuda.py:289] Using XFormers backend.\r\n",
      "[W829 04:57:30.298282290 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W829 04:57:31.648058608 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W829 04:57:40.308839059 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W829 04:57:50.319271986 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 08-29 04:57:51 [utils.py:1055] Found nccl from library libnccl.so.2\r\n",
      "INFO 08-29 04:57:51 [pynccl.py:69] vLLM is using nccl==2.21.5\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=467)\u001b[0;0m INFO 08-29 04:57:51 [utils.py:1055] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=467)\u001b[0;0m INFO 08-29 04:57:51 [pynccl.py:69] vLLM is using nccl==2.21.5\r\n",
      "INFO 08-29 04:57:51 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_7dc81fe2'), local_subscribe_addr='ipc:///tmp/67209e75-8fcf-4757-a259-b1938b6452b9', remote_subscribe_addr=None, remote_addr_ipv6=False)\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=467)\u001b[0;0m INFO 08-29 04:57:51 [parallel_state.py:1004] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\r\n",
      "INFO 08-29 04:57:51 [parallel_state.py:1004] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\r\n",
      "INFO 08-29 04:57:51 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=467)\u001b[0;0m INFO 08-29 04:57:51 [model_runner.py:1108] Starting to load model /kaggle/input/qwen2.5/transformers/32b-instruct-awq/1...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:01<00:04,  1.09s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:02<00:03,  1.25s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:37<00:33, 16.68s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:54<00:16, 16.71s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:56<00:00, 11.42s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:56<00:00, 11.26s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=467)\u001b[0;0m INFO 08-29 04:58:48 [loader.py:458] Loading weights took 56.47 seconds\r\n",
      "INFO 08-29 04:58:48 [loader.py:458] Loading weights took 56.49 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=467)\u001b[0;0m INFO 08-29 04:58:48 [model_runner.py:1140] Model loading took 9.0935 GiB and 56.775153 seconds\r\n",
      "INFO 08-29 04:58:48 [model_runner.py:1140] Model loading took 9.0935 GiB and 56.794246 seconds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=467)\u001b[0;0m INFO 08-29 04:59:26 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 2.06GiB; the rest of the memory reserved for KV Cache is 2.01GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 08-29 04:59:26 [worker.py:287] model weights take 9.09GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 2.06GiB; the rest of the memory reserved for KV Cache is 2.01GiB.\r\n",
      "INFO 08-29 04:59:26 [executor_base.py:112] # cuda blocks: 1031, # CPU blocks: 2048\r\n",
      "INFO 08-29 04:59:26 [executor_base.py:117] Maximum concurrency for 16384 tokens per request: 1.01x\r\n",
      "INFO 08-29 04:59:30 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 42.11 seconds\r\n",
      "[rank0]: Traceback (most recent call last):\r\n",
      "[rank0]:   File \"/tmp/src/predict.py\", line 151, in <module>\r\n",
      "[rank0]:     df = find_context_win(tokenizer,doi_df)\r\n",
      "[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/tmp/src/predict.py\", line 87, in find_context_win\r\n",
      "[rank0]:     text_df = pl.read_parquet('/tmp/context_data.parquet')\r\n",
      "[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/polars/_utils/deprecation.py\", line 92, in wrapper\r\n",
      "[rank0]:     return function(*args, **kwargs)\r\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/polars/_utils/deprecation.py\", line 92, in wrapper\r\n",
      "[rank0]:     return function(*args, **kwargs)\r\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/polars/io/parquet/functions.py\", line 241, in read_parquet\r\n",
      "[rank0]:     return lf.collect()\r\n",
      "[rank0]:            ^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/polars/lazyframe/frame.py\", line 2056, in collect\r\n",
      "[rank0]:     return wrap_df(ldf.collect(callback))\r\n",
      "[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]: FileNotFoundError: No such file or directory (os error 2): /tmp/context_data.parquet\r\n",
      "INFO 08-29 04:59:32 [multiproc_worker_utils.py:124] Killing local vLLM worker processes\r\n",
      "[rank0]:[W829 04:59:32.174020222 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "Exception ignored in: <Finalize object, dead>\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 227, in __call__\r\n",
      "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 206, in _finalize_close\r\n",
      "TypeError: 'NoneType' object is not callable\r\n",
      "INFO 2025-08-29 04:47:33  [getid.py:269 - main()] all - f1: 0.5543 [536/679/183]\r\n",
      "INFO 2025-08-29 04:47:33  [getid.py:269 - main()] doi - f1: 0.4316 [172/300/153]\r\n",
      "INFO 2025-08-29 04:47:33  [getid.py:269 - main()] acc - f1: 0.6403 [364/379/30]\r\n",
      "INFO 2025-08-29 04:47:33  [getid.py:271 - main()] all - f1: 0.4685 [453/762/266]\r\n",
      "INFO 2025-08-29 04:47:33  [getid.py:271 - main()] doi - f1: 0.3363 [134/338/191]\r\n",
      "INFO 2025-08-29 04:47:33  [getid.py:271 - main()] acc - f1: 0.5611 [319/424/75]\r\n",
      "INFO 2025-08-29 04:54:23  [llm_validate.py:182 - <module>()] all - f1: 0.6158 [517/443/202]\r\n",
      "INFO 2025-08-29 04:54:23  [llm_validate.py:182 - <module>()] doi - f1: 0.5646 [153/64/172]\r\n",
      "INFO 2025-08-29 04:54:23  [llm_validate.py:182 - <module>()] acc - f1: 0.6403 [364/379/30]\r\n",
      "INFO 2025-08-29 04:54:23  [llm_validate.py:185 - <module>()] all - f1: 0.5265 [442/518/277]\r\n",
      "INFO 2025-08-29 04:54:23  [llm_validate.py:185 - <module>()] doi - f1: 0.4539 [123/94/202]\r\n",
      "INFO 2025-08-29 04:54:23  [llm_validate.py:185 - <module>()] acc - f1: 0.5611 [319/424/75]\r\n",
      "INFO 2025-08-29 04:54:31  [post_filter.py:66 - main()] all - f1: 0.6134 [507/427/212]\r\n",
      "INFO 2025-08-29 04:54:31  [post_filter.py:66 - main()] doi - f1: 0.5543 [143/48/182]\r\n",
      "INFO 2025-08-29 04:54:31  [post_filter.py:66 - main()] acc - f1: 0.6403 [364/379/30]\r\n",
      "INFO 2025-08-29 04:54:31  [post_filter.py:67 - main()] all - f1: 0.5287 [437/497/282]\r\n",
      "INFO 2025-08-29 04:54:31  [post_filter.py:67 - main()] doi - f1: 0.4574 [118/73/207]\r\n",
      "INFO 2025-08-29 04:54:31  [post_filter.py:67 - main()] acc - f1: 0.5611 [319/424/75]\r\n"
     ]
    }
   ],
   "source": [
    "%cd /tmp\n",
    "!LOG_LEVEL=INFO python src/parse.py /tmp/train_parse\n",
    "! python src/check_parse.py\n",
    "! python src/getid.py\n",
    "! python src/llm_validate.py\n",
    "! python src/post_filter.py\n",
    "! python src/post_validate.py\n",
    "! python src/predict.py\n",
    "! grep \"f1:\" /tmp/logs/project.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854869f8",
   "metadata": {
    "papermill": {
     "duration": 0.032306,
     "end_time": "2025-08-29T04:59:36.676390",
     "exception": false,
     "start_time": "2025-08-29T04:59:36.644084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13015230,
     "sourceId": 82370,
     "sourceType": "competition"
    },
    {
     "sourceId": 248118764,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141565,
     "sourceId": 166368,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 875.601864,
   "end_time": "2025-08-29T04:59:37.026234",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-29T04:45:01.424370",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
